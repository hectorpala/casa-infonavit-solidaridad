# ü§ñ Sistema de Scraping Autom√°tico - Gu√≠a Completa

## üéØ Resumen

Sistema 100% autom√°tico para scrapear propiedades recientes de Inmuebles24 Culiac√°n. Funciona CON o SIN proxies.

**Tiempo de setup**: 5 minutos  
**Tiempo diario (manual)**: 3 min copiar URLs + 50 min autom√°tico = 53 min  
**Tiempo diario (auto)**: 0 min = 100% manos libres

---

## üöÄ Quick Start - Modo Manual ($0/mes)

### Paso 1: Configurar Archivo de URLs (3 minutos)

```bash
nano urls-propiedades-recientes-culiacan.txt
```

Pegar URLs de Inmuebles24 (una por l√≠nea), guardar con **Ctrl+X ‚Üí Y ‚Üí Enter**

### Paso 2: Ejecutar Script de Automatizaci√≥n

```bash
./daily-scraping.sh
```

¬°Eso es todo! El script:
- ‚úÖ Procesa todas las URLs
- ‚úÖ Limpia hist√≥rico antiguo
- ‚úÖ Genera estad√≠sticas
- ‚úÖ Publica autom√°ticamente a GitHub

**Resultado**: 20 propiedades procesadas en ~50 minutos, 100% autom√°tico despu√©s de copiar URLs.

---

## üî• Modo Autom√°tico CON Proxies ($5-49/mes)

### Paso 1: Configurar Credenciales

```bash
nano .env
```

Agregar tu API key de ScraperAPI (recomendado):

```bash
SCRAPERAPI_KEY=tu_api_key_aqui
```

O configurar otro proveedor (Oxylabs, BrightData, Zyte).

### Paso 2: Ejecutar Modo Auto

```bash
./daily-scraping.sh
```

El script detecta autom√°ticamente si hay proxies y:
- ‚úÖ Extrae URLs autom√°ticamente de Inmuebles24
- ‚úÖ Procesa con 3 workers paralelos
- ‚úÖ Publica todo autom√°ticamente

**Resultado**: 100% manos libres, ~60 propiedades/hora.

---

## ‚è∞ Configurar Cron Job (Ejecuci√≥n Diaria Autom√°tica)

### Paso 1: Abrir Crontab

```bash
crontab -e
```

### Paso 2: Agregar L√≠nea (Ejecutar todos los d√≠as a las 7 AM)

```bash
0 7 * * * /Users/hectorpc/Documents/Hector\ Palazuelos/Google\ My\ Business/landing\ casa\ solidaridad/daily-scraping.sh >> /tmp/daily-scraping-cron.log 2>&1
```

**Importante**: Cambiar la ruta a tu directorio real.

### Paso 3: Verificar Cron

```bash
crontab -l
```

¬°Listo! El sistema scrapear√° autom√°ticamente todos los d√≠as a las 7 AM.

---

## üìä Monitoreo y Logs

### Ver Logs del D√≠a

```bash
cat logs/daily-scraping-$(date +%Y%m%d).log
```

### Ver Estad√≠sticas del Hist√≥rico

```bash
node -e "
const PropertyHistory = require('./property-history');
const history = new PropertyHistory();
const stats = history.getStats();
console.log('üìä ESTAD√çSTICAS');
console.log('‚ïê'.repeat(50));
console.log(\`Total propiedades: \${stats.total}\`);
console.log(\`Recientes (‚â§20 d√≠as): \${stats.recent20}\`);
console.log(\`Cambios de precio: \${stats.priceChanges}\`);
console.log(\`Scrapeadas: \${stats.scraped}\`);
console.log('‚ïê'.repeat(50));
"
```

### Ver Resultados del Batch

```bash
# √öltimas 20 l√≠neas del log
tail -20 batch-scraping-log.txt

# Contar √©xitos vs errores
echo "√âxitos: $(grep -c SUCCESS batch-scraping-log.txt)"
echo "Errores: $(grep -c ERROR batch-scraping-log.txt)"
```

---

## üîß Configuraci√≥n Avanzada

### Cambiar Concurrencia (Workers Paralelos)

Editar `.env`:

```bash
# Sin proxies: 1-2 workers
MAX_CONCURRENT_WORKERS=1

# Con proxies: 3-5 workers
MAX_CONCURRENT_WORKERS=3
```

### Cambiar Delay Entre Propiedades

Editar `.env`:

```bash
# Sin proxies: 5-10 segundos recomendado
DELAY_BETWEEN_PROPS_MS=5000

# Con proxies: 2-5 segundos
DELAY_BETWEEN_PROPS_MS=2000
```

### Limpieza Autom√°tica del Hist√≥rico

Por defecto, el script limpia propiedades no vistas en 30+ d√≠as. Para cambiar:

Editar `daily-scraping.sh` l√≠nea 90:

```bash
const removed = history.cleanOldProperties(30);  # Cambiar 30 por d√≠as deseados
```

---

## üîî Notificaciones (Opcional)

### Configurar Webhook (Slack, Discord, etc.)

Editar `.env`:

```bash
NOTIFICATION_WEBHOOK=https://hooks.slack.com/services/YOUR/WEBHOOK/HERE
```

El script enviar√° notificaci√≥n al terminar con resumen de √©xitos/errores.

---

## üõ°Ô∏è Troubleshooting

### Problema: "Archivo urls-propiedades-recientes-culiacan.txt no encontrado"

**Soluci√≥n (Modo Manual)**:

```bash
nano urls-propiedades-recientes-culiacan.txt
# Pegar URLs, guardar
```

**Soluci√≥n (Modo Auto)**:

Configurar proxies en `.env` y ejecutar:

```bash
node extraer-urls-stealth.js
```

### Problema: Script cron no ejecuta

**Verificar permisos**:

```bash
chmod +x daily-scraping.sh
ls -l daily-scraping.sh  # Debe mostrar -rwxr-xr-x
```

**Verificar ruta en crontab**:

```bash
crontab -l  # Ver ruta actual
which node  # Verificar ruta de Node.js

# Si necesario, usar ruta completa:
0 7 * * * /usr/local/bin/node /ruta/completa/daily-scraping.sh
```

### Problema: Demasiados errores 429 (Rate Limited)

**Soluci√≥n**: Reducir concurrencia en `.env`:

```bash
MAX_CONCURRENT_WORKERS=1
DELAY_BETWEEN_PROPS_MS=10000  # 10 segundos
```

---

## üìà Comparaci√≥n de Modos

| Caracter√≠stica | Manual | Auto con Proxies |
|----------------|--------|------------------|
| **Costo** | $0/mes | $5-49/mes |
| **Tiempo setup** | 3 min/d√≠a | 0 min/d√≠a |
| **Tiempo total** | ~53 min/d√≠a | 0 min/d√≠a |
| **Propiedades/d√≠a** | 20-30 | 50-100+ |
| **Confiabilidad** | 100% | 95-98% |
| **Escalabilidad** | Limitada | Alta |
| **Cron friendly** | ‚ö†Ô∏è Requiere URLs | ‚úÖ 100% auto |

**Recomendaci√≥n**: Empezar con modo manual, escalar a auto si necesitas m√°s de 30 propiedades/d√≠a o 100% manos libres.

---

## üéØ Workflow T√≠pico

### D√≠a 1 (Setup)

```bash
# 1. Configurar .env (si quieres auto)
cp .env.example .env
nano .env  # Agregar API keys

# 2. Probar modo manual
nano urls-propiedades-recientes-culiacan.txt  # Copiar 2-3 URLs
./daily-scraping.sh

# 3. Verificar resultados
ls culiacan/  # Ver propiedades generadas
```

### D√≠a 2+ (Operaci√≥n)

**Modo Manual**:
```bash
# Copiar URLs nuevas
nano urls-propiedades-recientes-culiacan.txt

# Ejecutar
./daily-scraping.sh
```

**Modo Auto (con cron)**:
```bash
# No hacer nada - cron se encarga üéâ
# Revisar logs:
cat logs/daily-scraping-$(date +%Y%m%d).log
```

---

## üí° Tips y Mejores Pr√°cticas

1. **Backups**: El script no hace backups autom√°ticos. Considera:
   ```bash
   cp property-history.json backups/property-history-$(date +%Y%m%d).json
   ```

2. **Monitoreo de Costos**: Si usas ScraperAPI, revisa tu dashboard:
   - https://www.scraperapi.com/dashboard

3. **Optimizaci√≥n**: Ajusta `MAX_DAYS_RECENT` en `.env` seg√∫n necesidad:
   ```bash
   MAX_DAYS_RECENT=15  # Solo propiedades ‚â§15 d√≠as
   ```

4. **Testing**: Siempre probar en modo manual antes de configurar cron:
   ```bash
   ./daily-scraping.sh  # Ejecutar manualmente primero
   ```

5. **Logs Rotativos**: Los logs se guardan por fecha. Limpiar viejos:
   ```bash
   # Borrar logs de hace 30+ d√≠as
   find logs/ -name "daily-scraping-*.log" -mtime +30 -delete
   ```

---

## üìö Archivos Importantes

- `daily-scraping.sh` - Script principal de automatizaci√≥n
- `.env` - Configuraci√≥n (API keys, workers, delays)
- `property-history.json` - Hist√≥rico de propiedades
- `batch-scraping-log.txt` - Log del √∫ltimo batch
- `logs/daily-scraping-*.log` - Logs diarios rotados

---

## üÜò Soporte

Si tienes problemas:

1. **Revisar logs**: `cat logs/daily-scraping-$(date +%Y%m%d).log`
2. **Verificar configuraci√≥n**: `cat .env`
3. **Probar manualmente**: `./daily-scraping.sh`
4. **Ver documentaci√≥n completa**: `cat PROFESSIONAL-SCRAPING-SYSTEM.md`

---

**Sistema creado**: Octubre 2025  
**√öltima actualizaci√≥n**: Octubre 2025  
**Versi√≥n**: 3.0 Full Auto
