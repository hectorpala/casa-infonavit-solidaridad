# ‚úÖ CHECKLIST - Automatizaci√≥n Completa del Scraping

## üéØ Pre-requisitos

### 1. Configuraci√≥n Inicial
- [ ] Node.js instalado (v14+)
- [ ] Repositorio clonado localmente
- [ ] Dependencias instaladas: `npm install`
- [ ] Git configurado con permisos de push

### 2. Archivos Necesarios
- [ ] `.env` existe y tiene la estructura correcta
- [ ] `daily-scraping.sh` es ejecutable (`chmod +x`)
- [ ] Todos los scripts Node.js est√°n presentes:
  - [ ] `extraer-urls-stealth.js`
  - [ ] `scrapear-batch-urls.js`
  - [ ] `property-history.js`
  - [ ] `proxy-pool.js`
  - [ ] `inmuebles24culiacanscraper.js`

---

## üîß MODO MANUAL ($0/mes) - Sin Proxies

### Configuraci√≥n
- [ ] Archivo `.env` existe (puede estar vac√≠o o sin proxies)
- [ ] `urls-propiedades-recientes-culiacan.txt` creado

### Pasos de Verificaci√≥n

#### 1. Preparar URLs Manualmente
```bash
nano urls-propiedades-recientes-culiacan.txt
# Pegar 2-3 URLs de prueba
# Guardar: Ctrl+X ‚Üí Y ‚Üí Enter
```

**Verificar:**
- [ ] Archivo contiene URLs v√°lidas (una por l√≠nea)
- [ ] URLs comienzan con `https://www.inmuebles24.com/`

#### 2. Ejecutar Script Diario
```bash
./daily-scraping.sh
```

**Salida esperada:**
```
‚ÑπÔ∏è  Modo MANUAL (Sin proxies configurados)
‚ÑπÔ∏è  Para automatizaci√≥n completa, configura proxies en .env
‚ÑπÔ∏è  PASO 1: Extrayendo URLs de propiedades
‚úÖ Archivo de URLs encontrado: X URLs
‚ÑπÔ∏è  PASO 2: Limpiando hist√≥rico antiguo
‚úÖ Hist√≥rico limpio
‚ÑπÔ∏è  PASO 3: Procesando URLs con batch processor
‚ÑπÔ∏è  Concurrencia: 1 worker (modo MANUAL)
‚úÖ Batch processing completado
üìä ESTAD√çSTICAS FINALES
‚úÖ SCRAPING DIARIO COMPLETADO
```

**Verificar:**
- [ ] Script detecta "Modo MANUAL"
- [ ] Concurrencia = 1 worker
- [ ] Batch processor termina exitosamente
- [ ] Log generado en `logs/daily-scraping-YYYYMMDD.log`
- [ ] `batch-scraping-log.txt` contiene resultados

#### 3. Revisar Resultados
```bash
# Ver estad√≠sticas finales
cat logs/daily-scraping-$(date +%Y%m%d).log | grep -A 10 "ESTAD√çSTICAS"

# Contar √©xitos vs errores
echo "√âxitos: $(grep -c SUCCESS batch-scraping-log.txt)"
echo "Errores: $(grep -c ERROR batch-scraping-log.txt)"
```

**Verificar:**
- [ ] Total propiedades procesadas coincide con URLs
- [ ] Tasa de √©xito > 80%
- [ ] Errores documentados claramente

#### 4. Validar Property History
```bash
node -e "
const PropertyHistory = require('./property-history');
const history = new PropertyHistory();
const stats = history.getStats();
console.log('Total:', stats.total);
console.log('Recientes (‚â§20d):', stats.recent20);
console.log('Scrapeadas:', stats.scraped);
"
```

**Verificar:**
- [ ] Propiedades agregadas al hist√≥rico
- [ ] Duplicados detectados correctamente
- [ ] Timestamps `firstSeen` y `lastSeen` actualizados

---

## üöÄ MODO AUTO ($49/mes) - Con Proxies

### Configuraci√≥n ScraperAPI (Recomendado)

#### 1. Crear Cuenta
- [ ] Cuenta creada en https://www.scraperapi.com/signup
- [ ] Email verificado
- [ ] API Key copiada desde dashboard

#### 2. Configurar .env
```bash
nano .env
```

Agregar l√≠nea:
```
SCRAPERAPI_KEY=tu_api_key_de_50_caracteres
```

**Verificar:**
- [ ] API Key pegada completa (50+ caracteres)
- [ ] Sin espacios al inicio/final
- [ ] L√≠nea descomentada (sin `#` al inicio)

**Validar configuraci√≥n:**
```bash
grep SCRAPERAPI_KEY .env
# Debe mostrar: SCRAPERAPI_KEY=abc123...
```

#### 3. Verificar Detecci√≥n de Modo
```bash
./daily-scraping.sh 2>&1 | head -10
```

**Salida esperada:**
```
‚ÑπÔ∏è  Modo AUTO detectado (ScraperAPI configurado)
```

**Verificar:**
- [ ] Script detecta "Modo AUTO"
- [ ] No muestra advertencia de modo MANUAL

#### 4. Ejecutar Pipeline Completo
```bash
# Test r√°pido (sin ejecutar completo)
./daily-scraping.sh 2>&1 | head -50

# Ejecuci√≥n completa (cuando est√©s listo)
./daily-scraping.sh
```

**Proceso esperado:**
1. ‚úÖ **PASO 1**: Extractor ejecuta `extraer-urls-stealth.js`
   - Debe generar `urls-propiedades-recientes-culiacan.txt`
   - Contiene 20-40 URLs t√≠picamente
2. ‚úÖ **PASO 2**: Limpia hist√≥rico (>30 d√≠as)
3. ‚úÖ **PASO 3**: Batch processor con **concurrency 3**
   - Muestra "[Worker 1]", "[Worker 2]", "[Worker 3]" en paralelo
4. ‚úÖ **PASO 4**: Genera estad√≠sticas finales

**Verificar:**
- [ ] Extractor no falla (usa proxies correctamente)
- [ ] Batch processor muestra 3 workers en paralelo
- [ ] Tiempo total < 30 min para 30-40 propiedades
- [ ] Log completo en `logs/daily-scraping-YYYYMMDD.log`

#### 5. Monitorear Uso de ScraperAPI
- [ ] Login en https://www.scraperapi.com/dashboard
- [ ] Verificar requests usados hoy
- [ ] Confirmar tasa de √©xito > 95%
- [ ] Revisar costos del mes

**Plan Gratuito (1000 requests):**
- ~30-40 propiedades = 100-150 requests
- Plan gratuito dura ~6-7 ejecuciones diarias

**Plan Hobby ($49/mo, 100K requests):**
- ~3,000 propiedades/mes
- ~100 propiedades/d√≠a sin preocupaciones

---

## ‚è∞ AUTOMATIZACI√ìN CON CRON

### 1. Preparar Entrada de Cron
```bash
# Obtener ruta absoluta del script
SCRIPT_PATH="$(cd "$(dirname "$0")" && pwd)/daily-scraping.sh"
echo $SCRIPT_PATH
```

### 2. Editar Crontab
```bash
crontab -e
```

**Agregar l√≠nea (ejecutar a las 7 AM diariamente):**
```cron
0 7 * * * /Users/hectorpc/Documents/Hector\ Palazuelos/Google\ My\ Business/landing\ casa\ solidaridad/daily-scraping.sh >> /tmp/daily-scraping-cron.log 2>&1
```

**‚ö†Ô∏è IMPORTANTE:** Reemplazar la ruta con tu `$SCRIPT_PATH` real.

**Otros horarios comunes:**
```cron
0 7 * * *     # 7:00 AM diario
0 12 * * *    # 12:00 PM diario
0 19 * * *    # 7:00 PM diario
0 7 * * 1     # 7:00 AM solo lunes
0 7 * * 1,4   # 7:00 AM lunes y jueves
```

### 3. Verificar Cron Configurado
```bash
crontab -l
```

**Verificar:**
- [ ] L√≠nea agregada correctamente
- [ ] Ruta absoluta sin espacios problem√°ticos
- [ ] Output redirigido a `/tmp/daily-scraping-cron.log`

### 4. Probar Ejecuci√≥n Manual (Simular Cron)
```bash
# Ejecutar como si fuera cron
/bin/sh -c "cd /path/to/landing\ casa\ solidaridad && ./daily-scraping.sh >> /tmp/test-cron.log 2>&1"

# Ver resultado
tail -50 /tmp/test-cron.log
```

**Verificar:**
- [ ] Script ejecuta correctamente desde cron context
- [ ] Sin errores de permisos
- [ ] Sin errores de rutas relativas

### 5. Monitorear Ejecuciones Diarias
```bash
# Ver log del cron m√°s reciente
tail -100 /tmp/daily-scraping-cron.log

# Ver logs diarios del script
ls -lht logs/daily-scraping-*.log | head -5
cat logs/daily-scraping-$(date +%Y%m%d).log
```

**Verificar cada semana:**
- [ ] Cron ejecuta a la hora correcta
- [ ] Logs generados diariamente
- [ ] Tasa de √©xito consistente
- [ ] Property history crece progresivamente

---

## üìä VALIDACI√ìN FINAL

### Modo Manual - Checklist Completo
- [ ] ‚úÖ Script detecta "Modo MANUAL"
- [ ] ‚úÖ Concurrency = 1 worker
- [ ] ‚úÖ Procesa URLs desde archivo TXT
- [ ] ‚úÖ Genera log exitosamente
- [ ] ‚úÖ Property history actualizado
- [ ] ‚úÖ Duplicados detectados
- [ ] ‚úÖ Tiempo: ~50 min para 20 propiedades

### Modo Auto - Checklist Completo
- [ ] ‚úÖ Script detecta "Modo AUTO"
- [ ] ‚úÖ SCRAPERAPI_KEY configurado correctamente
- [ ] ‚úÖ Extractor genera URLs autom√°ticamente
- [ ] ‚úÖ Concurrency = 3 workers (paralelo)
- [ ] ‚úÖ Workers se ejecutan simult√°neamente
- [ ] ‚úÖ Genera log completo
- [ ] ‚úÖ Property history actualizado
- [ ] ‚úÖ Dashboard ScraperAPI muestra uso
- [ ] ‚úÖ Tiempo: ~20-30 min para 30-40 propiedades

### Cron - Checklist Completo
- [ ] ‚úÖ Crontab configurado correctamente
- [ ] ‚úÖ Ejecuta a la hora programada
- [ ] ‚úÖ Logs generados en `/tmp/` y `logs/`
- [ ] ‚úÖ Sin errores de permisos
- [ ] ‚úÖ Property history crece diariamente

---

## üÜò TROUBLESHOOTING

### Problema: Modo AUTO no detectado
**S√≠ntomas:** Script muestra "Modo MANUAL" aunque hay API key

**Soluci√≥n:**
```bash
# 1. Verificar archivo .env existe
ls -la .env

# 2. Verificar contenido
cat .env | grep SCRAPERAPI_KEY

# 3. Verificar formato correcto
grep -v "^#" .env | grep "SCRAPERAPI_KEY" | grep "=."
```

**Causas comunes:**
- API key vac√≠a: `SCRAPERAPI_KEY=`
- L√≠nea comentada: `#SCRAPERAPI_KEY=...`
- Espacios extra: `SCRAPERAPI_KEY = abc123`

---

### Problema: Batch processor no usa concurrencia
**S√≠ntomas:** Solo muestra 1 worker aunque modo AUTO detectado

**Soluci√≥n:**
```bash
# 1. Verificar que daily-scraping.sh pasa el flag
grep "scrapear-batch-urls.js" daily-scraping.sh

# Debe mostrar:
# node scrapear-batch-urls.js --concurrency $CONCURRENCY

# 2. Probar manualmente
node scrapear-batch-urls.js --concurrency 3 --test 3

# Debe mostrar:
# ‚öôÔ∏è  Configuraci√≥n:
#    ‚Ä¢ Concurrencia: 3 workers
```

---

### Problema: Extractor falla con proxies
**S√≠ntomas:** "Extractor fall√≥. Continuando con URLs existentes..."

**Soluci√≥n:**
```bash
# 1. Verificar API key v√°lida
curl "http://api.scraperapi.com/account?api_key=TU_API_KEY"

# Debe retornar JSON con:
# {"requestCount": X, "requestLimit": 1000}

# 2. Probar extractor manualmente
node extraer-urls-stealth.js

# 3. Revisar errores espec√≠ficos en log
cat logs/daily-scraping-$(date +%Y%m%d).log | grep -i error
```

**Causas comunes:**
- API key inv√°lida
- Plan gratuito agotado (1000 requests)
- Inmuebles24 cambi√≥ estructura HTML

---

### Problema: Cron no ejecuta
**S√≠ntomas:** No hay logs en `/tmp/daily-scraping-cron.log`

**Soluci√≥n:**
```bash
# 1. Verificar cron est√° corriendo
ps aux | grep cron

# 2. Verificar permisos del script
ls -l daily-scraping.sh
# Debe mostrar: -rwxr-xr-x

# 3. Hacer ejecutable si no lo es
chmod +x daily-scraping.sh

# 4. Probar ejecuci√≥n desde /bin/sh
/bin/sh -c "./daily-scraping.sh" 2>&1 | head -20

# 5. Verificar logs de sistema
grep CRON /var/log/system.log | tail -20  # macOS
```

---

## üìà OPTIMIZACI√ìN

### Ajustar Concurrencia
```bash
nano .env
```

**Sin proxies:**
```
MAX_CONCURRENT_WORKERS=1   # Recomendado
```

**Con proxies (Plan Gratuito):**
```
MAX_CONCURRENT_WORKERS=2   # Conservador
```

**Con proxies (Plan Hobby):**
```
MAX_CONCURRENT_WORKERS=3   # Balanceado (default)
MAX_CONCURRENT_WORKERS=5   # Agresivo (si tienes muchas URLs)
```

### Ajustar Delays
```bash
nano .env
```

**Sin proxies:**
```
DELAY_BETWEEN_PROPS_MS=10000  # 10 segundos (conservador)
```

**Con proxies:**
```
DELAY_BETWEEN_PROPS_MS=2000   # 2 segundos (agresivo)
DELAY_BETWEEN_PROPS_MS=5000   # 5 segundos (balanceado)
```

---

## üéØ RESUMEN FINAL

### Estado Actual del Sistema

**Archivos clave:**
- ‚úÖ `.env` - Configuraci√≥n de proxies y workers
- ‚úÖ `daily-scraping.sh` - Orquestador principal (ejecutable)
- ‚úÖ `scrapear-batch-urls.js` - Batch processor con soporte concurrencia
- ‚úÖ `property-history.js` - Tracking de propiedades con deduplicaci√≥n
- ‚úÖ `proxy-pool.js` - Pool de proxies rotativos

**Modos operativos:**
1. **MANUAL ($0/mes)** - Copiar URLs manualmente, 1 worker, ~50 min
2. **AUTO ($49/mes)** - Extracci√≥n autom√°tica, 3 workers, ~25 min
3. **CRON** - 100% manos libres, ejecuci√≥n diaria programada

**Pr√≥ximos pasos:**
1. [ ] Configurar .env con SCRAPERAPI_KEY
2. [ ] Probar modo AUTO con `./daily-scraping.sh`
3. [ ] Validar workers paralelos en logs
4. [ ] Configurar cron para ejecuci√≥n diaria
5. [ ] Monitorear dashboard ScraperAPI semanalmente

**Documentaci√≥n adicional:**
- `SCRAPERAPI-SETUP.md` - Gu√≠a r√°pida ScraperAPI (5 min)
- `AUTOMATION-README.md` - Documentaci√≥n completa (15 KB)
- `PROFESSIONAL-SCRAPING-SYSTEM.md` - Arquitectura t√©cnica

---

**Sistema creado:** Octubre 2025
**√öltima actualizaci√≥n:** Octubre 2025
**Versi√≥n:** 3.0 Full Auto + Concurrency
