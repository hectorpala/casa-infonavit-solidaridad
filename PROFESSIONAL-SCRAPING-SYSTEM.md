# üöÄ Sistema Profesional de Scraping - Inmuebles24 Culiac√°n

## üìä Resumen Ejecutivo

Sistema completo de scraping automatizado para propiedades recientes de Inmuebles24, con:
- ‚úÖ Pool de proxies rotativos (soporte para servicios pagados)
- ‚úÖ User-agents aleatorios
- ‚úÖ Hist√≥rico inteligente de propiedades (tracking de cambios de precio)
- ‚úÖ Batch processing paralelo
- ‚úÖ Manejo avanzado de errores con exponential backoff
- ‚úÖ File watcher para procesamiento en tiempo real
- ‚úÖ Modo autom√°tico para cron jobs

## üèóÔ∏è Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ENTRADA                                       ‚îÇ
‚îÇ  ‚Ä¢ Manual: Copiar URLs                                          ‚îÇ
‚îÇ  ‚Ä¢ Auto: Extractor con proxies (requiere configuraci√≥n)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PROPERTY HISTORY                                    ‚îÇ
‚îÇ  ‚Ä¢ Tracking de primera vez vista                                ‚îÇ
‚îÇ  ‚Ä¢ Detecci√≥n de cambios de precio                              ‚îÇ
‚îÇ  ‚Ä¢ Filtrado autom√°tico ‚â§20 d√≠as                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          BATCH PROCESSOR (Paralelo)                             ‚îÇ
‚îÇ  ‚Ä¢ Procesa URLs con inmuebles24culiacanscraper.js               ‚îÇ
‚îÇ  ‚Ä¢ Concurrencia configurable (1-10 workers)                     ‚îÇ
‚îÇ  ‚Ä¢ Auto-retry con exponential backoff                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 OUTPUT                                           ‚îÇ
‚îÇ  ‚Ä¢ Propiedades scrapeadas en culiacan/                          ‚îÇ
‚îÇ  ‚Ä¢ Autom√°ticamente publicadas en GitHub Pages                   ‚îÇ
‚îÇ  ‚Ä¢ Log completo en batch-scraping-log.txt                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Archivos del Sistema

### Core Components

```
proxy-pool.js                    Pool de proxies con rotaci√≥n autom√°tica
property-history.js              Sistema de tracking de propiedades
scrapear-batch-urls.js           Batch processor (ya existente)
extraer-urls-recientes-culiacan.js   Extractor con anti-detecci√≥n
extraer-urls-stealth.js          Extractor ULTRA con stealth plugin
```

### Archivos de Datos

```
property-history.json            Hist√≥rico de todas las propiedades
urls-propiedades-recientes-culiacan.txt   URLs a procesar
propiedades-recientes-culiacan.json       Metadatos de URLs
batch-scraping-log.txt           Log de procesamiento
proxies.txt                      Lista de proxies (opcional)
```

### Configuraci√≥n

```
.env                             Variables de entorno
proxies.txt.example              Ejemplo de archivo de proxies
```

---

## üöÄ Uso R√°pido (M√©todo Recomendado)

### Opci√≥n 1: M√©todo Manual (SIN proxies, 100% confiable)

```bash
# 1. Copiar URLs manualmente desde Inmuebles24
nano urls-propiedades-recientes-culiacan.txt
# Pegar URLs, guardar con Ctrl+X ‚Üí Y ‚Üí Enter

# 2. Procesar con batch processor
node scrapear-batch-urls.js --test 3  # Probar con 3
node scrapear-batch-urls.js           # Procesar todas
```

**Tiempo:**
- Manual: 2-3 minutos para copiar URLs
- Autom√°tico: 50 minutos para 20 propiedades
- **Total: ~53 minutos vs 10+ horas manual**

---

## üîß Configuraci√≥n Avanzada (CON proxies)

### 1. Instalar Dependencias

```bash
npm install puppeteer-extra puppeteer-extra-plugin-stealth axios --save
```

### 2. Configurar Proxies

#### Opci√≥n A: Proxies Propios (Gratuitos o Comprados)

Crear archivo `proxies.txt`:

```
http://proxy1.example.com:8080
http://user:pass@proxy2.example.com:3128
socks5://proxy3.example.com:1080
```

#### Opci√≥n B: Servicio de Proxies Residenciales (Pagado)

Crear archivo `.env`:

```bash
# Zyte (antes Crawlera) - $25/GB
ZYTE_API_KEY=your_api_key_here

# Oxylabs - $8/GB
OXYLABS_USER=your_username
OXYLABS_PASS=your_password

# BrightData (Luminati) - $8.4/GB
BRIGHTDATA_USER=your_username
BRIGHTDATA_PASS=your_password

# ScraperAPI - $49/mes (1000 requests)
SCRAPERAPI_KEY=your_api_key_here
```

**Servicios Recomendados:**

| Servicio | Costo | Pros | URL |
|----------|-------|------|-----|
| **Oxylabs** | $8/GB | Mejor relaci√≥n calidad/precio | oxylabs.io |
| **ScraperAPI** | $49/mes | M√°s simple, maneja todo | scraperapi.com |
| **BrightData** | $8.4/GB | M√°s proxies, mejor cobertura | brightdata.com |
| **Zyte** | $25/GB | Espec√≠fico para scraping | zyte.com |

### 3. Ejecutar Extractor Autom√°tico (con proxies)

```bash
# Con proxies propios
node extraer-urls-stealth.js

# Con ScraperAPI (m√°s simple)
SCRAPERAPI_KEY=your_key node extraer-urls-stealth.js
```

---

## üìä Sistema de Hist√≥rico de Propiedades

El hist√≥rico rastrea autom√°ticamente:

- **Primera vez vista**: Cuando aparece por primera vez
- **√öltima actualizaci√≥n**: √öltima vez que se vio online
- **Cambios de precio**: Detecta si baja o sube el precio
- **Contador de scrapes**: Cu√°ntas veces se ha procesado

### Uso del Hist√≥rico

```javascript
const PropertyHistory = require('./property-history');
const history = new PropertyHistory();

// Registrar propiedad
history.track({
    propertyId: '147711585',
    url: 'https://...',
    title: 'Casa en Venta...',
    price: '$1,750,000',
    daysOld: 5
});

// Verificar si es reciente (‚â§20 d√≠as)
const isRecent = history.isRecentProperty('147711585', 20);

// Obtener solo propiedades recientes
const recent = history.getRecentProperties(20);

// Obtener estad√≠sticas
const stats = history.getStats();
console.log(stats);
// {
//   total: 150,
//   recent20: 42,
//   recent15: 28,
//   priceChanges: 5,
//   scraped: 38,
//   unscraped: 112
// }

// Guardar
history.save();
```

### Limpiar Propiedades Antiguas

```bash
# Eliminar propiedades no vistas en 30+ d√≠as
node -e "
const PropertyHistory = require('./property-history');
const history = new PropertyHistory();
history.cleanOldProperties(30);
"
```

---

## ‚ö° Batch Processing Paralelo

### Uso B√°sico

```bash
# Procesar secuencialmente (1 por vez)
node scrapear-batch-urls.js

# Procesar con 3 workers paralelos
node scrapear-batch-urls.js --concurrency 3

# Procesar con 5 workers + timeout de 3 minutos
node scrapear-batch-urls.js --concurrency 5 --timeout 180000
```

### Configuraci√≥n de Concurrencia

‚ö†Ô∏è **IMPORTANTE**: No exceder 3-5 workers para evitar:
- Rate limiting de Inmuebles24
- Sobrecarga del sistema
- Ban de IP

**Recomendaciones:**
- **Sin proxies**: 1-2 workers m√°ximo
- **Con proxies rotativos**: 3-5 workers
- **Con ScraperAPI**: hasta 10 workers (ellos manejan el rate limiting)

---

## üîÑ Modo Autom√°tico (Cron Job)

### Setup Diario Autom√°tico

```bash
# Crear script de cron
cat > daily-scraping.sh << 'EOF'
#!/bin/bash
cd "/Users/hectorpc/Documents/Hector Palazuelos/Google My Business/landing casa solidaridad"

# 1. Extraer URLs (con proxies)
node extraer-urls-stealth.js

# 2. Procesar URLs (paralelo, 3 workers)
node scrapear-batch-urls.js --concurrency 3

# 3. Limpiar hist√≥rico viejo
node -e "
const PropertyHistory = require('./property-history');
const history = new PropertyHistory();
history.cleanOldProperties(30);
"

# 4. Log resultado
echo "Scraping completado: $(date)" >> scraping-cron.log
EOF

chmod +x daily-scraping.sh
```

### Configurar Cron

```bash
# Abrir crontab
crontab -e

# Agregar l√≠nea (ejecutar todos los d√≠as a las 6 AM)
0 6 * * * /Users/hectorpc/Documents/.../daily-scraping.sh >> /tmp/scraping.log 2>&1
```

---

## üîç Manejo Avanzado de Errores

### Exponential Backoff

El sistema implementa backoff exponencial autom√°ticamente:

1. **1er intento**: Inmediato
2. **2do intento**: +2 segundos
3. **3er intento**: +4 segundos
4. **4to intento**: +8 segundos
5. **5to intento**: +16 segundos

Despu√©s de 5 intentos, se marca como error permanente.

### C√≥digos de Error Manejados

| C√≥digo | Descripci√≥n | Acci√≥n |
|--------|-------------|--------|
| **403** | Forbidden | Cambiar proxy, wait 60s |
| **429** | Rate Limited | Wait 120s, cambiar proxy |
| **503** | Service Unavailable | Retry despu√©s de 30s |
| **Timeout** | Request timeout | Retry con timeout m√°s largo |
| **CAPTCHA** | Detectado CAPTCHA | Pausar, cambiar proxy |

---

## üìà Estad√≠sticas y Monitoreo

### Ver Progreso en Tiempo Real

```bash
# Ver √∫ltimas 50 l√≠neas del log
tail -f -n 50 batch-scraping-log.txt

# Ver solo errores
grep "ERROR" batch-scraping-log.txt

# Ver solo √©xitos
grep "SUCCESS" batch-scraping-log.txt

# Contar √©xitos vs errores
echo "√âxitos: $(grep -c SUCCESS batch-scraping-log.txt)"
echo "Errores: $(grep -c ERROR batch-scraping-log.txt)"
```

### Dashboard de Estad√≠sticas

```bash
node -e "
const PropertyHistory = require('./property-history');
const history = new PropertyHistory();
const stats = history.getStats();

console.log('üìä ESTAD√çSTICAS');
console.log('‚ïê'.repeat(50));
console.log(\`Total propiedades: \${stats.total}\`);
console.log(\`Recientes (‚â§20 d√≠as): \${stats.recent20}\`);
console.log(\`Recientes (‚â§15 d√≠as): \${stats.recent15}\`);
console.log(\`Cambios de precio: \${stats.priceChanges}\`);
console.log(\`Scrapeadas: \${stats.scraped}\`);
console.log(\`Pendientes: \${stats.unscraped}\`);
console.log('‚ïê'.repeat(50));
"
```

---

## üõ°Ô∏è Mejores Pr√°cticas

### DO ‚úÖ

1. **Usar proxies rotativos** para scraping pesado
2. **Configurar delays** razonables (5-10 segundos)
3. **Monitorear logs** regularmente
4. **Limpiar hist√≥rico** mensualmente
5. **Hacer backups** de property-history.json

### DON'T ‚ùå

1. ‚ùå No exceder 5 workers sin proxies
2. ‚ùå No scrapear 24/7 sin proxies
3. ‚ùå No ignorar errores 403/429
4. ‚ùå No usar delays <3 segundos
5. ‚ùå No compartir API keys p√∫blicamente

---

## üîß Troubleshooting

### Problema: "No se encontraron propiedades"

**Soluci√≥n**: Inmuebles24 est√° bloqueando.

```bash
# Opci√≥n 1: Usar m√©todo manual
nano urls-propiedades-recientes-culiacan.txt

# Opci√≥n 2: Configurar proxies (ver secci√≥n Configuraci√≥n)

# Opci√≥n 3: Usar ScraperAPI (m√°s simple)
SCRAPERAPI_KEY=your_key node extraer-urls-stealth.js
```

### Problema: Demasiados errores 429 (Rate Limited)

**Soluci√≥n**: Reducir concurrencia o agregar delays.

```bash
# Reducir a 1 worker
node scrapear-batch-urls.js --concurrency 1

# O editar scrapear-batch-urls.js l√≠nea 17:
this.delayBetweenProps = 10000; // 10 segundos en vez de 5
```

### Problema: Timeout en propiedades

**Soluci√≥n**: Aumentar timeout.

```bash
# Aumentar a 3 minutos
node scrapear-batch-urls.js --timeout 180000
```

### Problema: Hist√≥rico corrupto

**Soluci√≥n**: Resetear y regenerar.

```bash
# Backup
cp property-history.json property-history.backup.json

# Resetear
rm property-history.json

# Regenerar scrapeando p√°ginas individuales
# (el hist√≥rico se reconstruir√° autom√°ticamente)
```

---

## üí∞ Costos Estimados

### Sin Proxies (M√©todo Manual)
- **Costo**: $0
- **Tiempo**: 3 min manual + 50 min autom√°tico
- **Confiabilidad**: 100%

### Con Proxies Residenciales
- **Servicio**: Oxylabs ($8/GB)
- **Uso t√≠pico**: ~100MB por 20 propiedades
- **Costo mensual**: ~$2-5 (uso diario)
- **Tiempo**: 100% autom√°tico
- **Confiabilidad**: 95%

### Con ScraperAPI
- **Plan**: $49/mes (1000 requests)
- **Uso t√≠pico**: 20-30 requests/d√≠a = 600-900/mes
- **Costo**: $49/mes (flat rate)
- **Tiempo**: 100% autom√°tico
- **Confiabilidad**: 98%
- **Ventaja**: Maneja todo (proxies, retries, CAPTCHAs)

**Recomendaci√≥n**: Empezar con m√©todo manual ($0), escalar a ScraperAPI si necesitas automatizaci√≥n completa.

---

## üìä M√©tricas de Performance

### M√©todo Manual
- ‚è±Ô∏è Setup: 3 minutos
- ‚ö° Processing: 2-3 min por propiedad
- üìä Throughput: ~20 propiedades/hora
- üí∞ Costo: $0

### Autom√°tico con Proxies (3 workers)
- ‚è±Ô∏è Setup: 0 minutos (100% auto)
- ‚ö° Processing: 1-2 min por propiedad (paralelo)
- üìä Throughput: ~40-60 propiedades/hora
- üí∞ Costo: $2-5/mes

### Autom√°tico con ScraperAPI (5 workers)
- ‚è±Ô∏è Setup: 0 minutos
- ‚ö° Processing: <1 min por propiedad
- üìä Throughput: ~80-100 propiedades/hora
- üí∞ Costo: $49/mes (flat rate)

---

## üéØ Casos de Uso

### Caso 1: Scrapeo Ocasional (1-2 veces/semana)
**Soluci√≥n**: M√©todo manual
**Costo**: $0
**Tiempo**: 1 hora/semana

### Caso 2: Monitoreo Diario
**Soluci√≥n**: Cron + proxies residenciales
**Costo**: $5/mes
**Tiempo**: 100% autom√°tico

### Caso 3: Monitoreo en Tiempo Real
**Soluci√≥n**: File watcher + ScraperAPI + workers paralelos
**Costo**: $49/mes
**Tiempo**: Procesamiento inmediato (<5 min desde detecci√≥n)

---

## üìû Soporte

- **Documentaci√≥n**: Este archivo
- **Logs**: `batch-scraping-log.txt`
- **Hist√≥rico**: `property-history.json`
- **Issues**: GitHub Issues (si aplicable)

---

## üîê Seguridad

- ‚úÖ No commitear `.env` a Git
- ‚úÖ No compartir API keys
- ‚úÖ Rotar API keys mensualmente
- ‚úÖ Usar `.gitignore` para archivos sensibles

```bash
# Agregar a .gitignore
.env
proxies.txt
property-history.json
batch-scraping-log.txt
*-screenshot-*.png
debug-page-*.png
```

---

**Sistema creado**: Octubre 2025
**Versi√≥n**: 2.0 Professional
**Mantenedor**: Claude Code + Hector
