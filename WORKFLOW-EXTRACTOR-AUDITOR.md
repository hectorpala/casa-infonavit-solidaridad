# ğŸ”„ Workflow: Extractor â†’ Auditor â†’ Scraper

Pipeline completo para extracciÃ³n, auditorÃ­a y scraping de propiedades de Inmuebles24.

---

## ğŸ“‹ Ãndice

1. [VisiÃ³n General](#visiÃ³n-general)
2. [Paso 1: Extractor de URLs](#paso-1-extractor-de-urls)
3. [Paso 2: Auditor de URLs](#paso-2-auditor-de-urls)
4. [Paso 3: Scraper Masivo](#paso-3-scraper-masivo)
5. [HistÃ³rico y AnÃ¡lisis](#histÃ³rico-y-anÃ¡lisis)
6. [Casos de Uso](#casos-de-uso)

---

## ğŸ¯ VisiÃ³n General

### Arquitectura del Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASO 1: EXTRACTOR                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  Input:  URL de bÃºsqueda Inmuebles24                            â”‚
â”‚  Output: urls-inmuebles24-YYYY-MM-DD-HH-MM-SS-valid.txt         â”‚
â”‚          urls-inmuebles24-YYYY-MM-DD-HH-MM-SS.json              â”‚
â”‚                                                                  â”‚
â”‚  â€¢ Extrae URLs de mÃºltiples pÃ¡ginas                             â”‚
â”‚  â€¢ Verifica HTTP (HEAD requests)                                â”‚
â”‚  â€¢ Separa vÃ¡lidas/bloqueadas/removidas                          â”‚
â”‚  â€¢ Genera metadata (timestamp, criterio, stats)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASO 2: AUDITOR                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  Input:  urls-inmuebles24-...-valid.txt                         â”‚
â”‚  Output: audit-YYYY-MM-DD-HH-MM-SS-nuevas.txt                   â”‚
â”‚          audit-YYYY-MM-DD-HH-MM-SS.json                         â”‚
â”‚          audit-history.csv (acumulativo)                        â”‚
â”‚                                                                  â”‚
â”‚  â€¢ Detecta duplicados vs base de datos                          â”‚
â”‚  â€¢ Re-verifica URLs tumbadas (opcional)                         â”‚
â”‚  â€¢ Auto-descarta URLs obsoletas (opcional)                      â”‚
â”‚  â€¢ Genera histÃ³rico CSV de corridas                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASO 3: SCRAPER MASIVO                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚  Input:  audit-...-nuevas.txt                                   â”‚
â”‚  Output: Propiedades publicadas en casasenventa.info            â”‚
â”‚          inmuebles24-scraped-properties.json (actualizado)      â”‚
â”‚                                                                  â”‚
â”‚  â€¢ Scrapea solo URLs nuevas y vÃ¡lidas                           â”‚
â”‚  â€¢ Descarga fotos, genera HTML, publica GitHub                  â”‚
â”‚  â€¢ Actualiza base de datos de duplicados                        â”‚
â”‚  â€¢ Actualiza CRM de vendedores                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ventajas del Pipeline

âœ… **Trazabilidad completa** - Cada paso genera archivos con timestamp
âœ… **Evita duplicados** - Auditor detecta propiedades ya scrapeadas
âœ… **Detecta URLs obsoletas** - Verifica HTTP antes de scrapear
âœ… **HistÃ³rico acumulativo** - CSV con todas las corridas
âœ… **OptimizaciÃ³n de recursos** - Solo scrapea URLs nuevas y vÃ¡lidas
âœ… **AnÃ¡lisis de rendimiento** - MÃ©tricas de cada corrida

---

## ğŸ” Paso 1: Extractor de URLs

### Comando

```bash
node 1extractorurlinmuebles24.js "URL_BUSQUEDA" [--max-pages N]
```

### Ejemplo

```bash
# Extraer 5 pÃ¡ginas de casas en venta en CuliacÃ¡n
node 1extractorurlinmuebles24.js \
  "https://www.inmuebles24.com/casas-en-venta-en-culiacan.html" \
  --max-pages 5
```

### Salida Generada

```
urls-inmuebles24-2025-10-26-16-35-21.json         (metadata completa)
urls-inmuebles24-2025-10-26-16-35-21-valid.txt    (URLs usables)
urls-inmuebles24-2025-10-26-16-35-21-removed.txt  (URLs removidas, si existen)
```

### Contenido JSON

```json
{
  "metadata": {
    "timestampISO": "2025-10-26T23:35:21.349Z",
    "timestampReadable": "26 de octubre de 2025, 04:35:21 p.m.",
    "searchCriteria": {
      "type": "Casas",
      "operation": "Venta",
      "city": "CuliacÃ¡n",
      "priceRange": "$3,500,000 - $3,550,000",
      "fullUrl": "https://www.inmuebles24.com/..."
    },
    "pagesProcessed": 5
  },
  "statistics": {
    "totalExtracted": 150,
    "validUrls": 10,
    "blockedUrls": 120,
    "totalUsableUrls": 130,
    "removedUrls": 15,
    "errorUrls": 5,
    "usablePercentage": "86.67%"
  },
  "urls": {
    "valid": [...],
    "blocked": [...],
    "removed": [...],
    "errors": [...]
  }
}
```

### InterpretaciÃ³n de Resultados

- **valid**: URLs con HTTP 200-399 (verificadas como activas)
- **blocked**: URLs con HTTP 403 (Cloudflare, probablemente vÃ¡lidas)
- **removed**: URLs que redirigen al home (propiedades removidas)
- **errors**: URLs con errores de red o timeout

**Nota:** URLs bloqueadas se incluyen en `-valid.txt` porque son probablemente vÃ¡lidas (el scraper completo las manejarÃ¡).

---

## ğŸ“Š Paso 2: Auditor de URLs

### Comandos

```bash
# BÃ¡sico (verifica HTTP y detecta duplicados)
node auditor-urls-inmuebles24.js urls-inmuebles24-...-valid.txt

# Auto-descartar URLs tumbadas
node auditor-urls-inmuebles24.js urls-inmuebles24-...-valid.txt --auto-discard

# Omitir verificaciÃ³n HTTP (mÃ¡s rÃ¡pido)
node auditor-urls-inmuebles24.js urls-inmuebles24-...-valid.txt --skip-http-check
```

### Ejemplo

```bash
node auditor-urls-inmuebles24.js \
  urls-inmuebles24-2025-10-26-16-35-21-valid.txt \
  --auto-discard
```

### Salida Generada

```
audit-2025-10-26-16-41-36.json         (resultados detallados)
audit-2025-10-26-16-41-36-nuevas.txt   (URLs listas para scrapear)
audit-history.csv                       (histÃ³rico acumulativo)
```

### Output Console

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“Š AUDITOR DE URLs - INMUEBLES24                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Archivo de entrada: urls-inmuebles24-2025-10-26-16-35-21-valid.txt
âš™ï¸  Auto-descartar URLs tumbadas: SÃ
âš™ï¸  VerificaciÃ³n HTTP: ACTIVA

ğŸ“Š URLs cargadas: 130

ğŸ“š Cargando base de datos de propiedades scrapeadas...
   âœ… 234 propiedades en base de datos

ğŸ” Clasificando URLs...
   âœ… URLs nuevas: 85
   âš ï¸  URLs duplicadas: 45

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ” VERIFICACIÃ“N HTTP DE URLs                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Verificando 85 URLs...

   â³ Progreso: 85/85 URLs verificadas...

ğŸ“Š RESULTADOS VERIFICACIÃ“N HTTP:
   âœ… URLs OK: 12
   ğŸ”’ URLs bloqueadas (Cloudflare): 68
   âŒ URLs tumbadas (404/redirect): 5
   âš ï¸  Errores de red: 0

ğŸ—‘ï¸  AUTO-DESCARTE ACTIVADO...
   âŒ URLs descartadas: 5

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“Š RESUMEN DE AUDITORÃA                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š CLASIFICACIÃ“N FINAL:
   ğŸ†• URLs nuevas usables: 80
   âŒ URLs nuevas tumbadas: 0
   âš ï¸  URLs duplicadas: 45
   ğŸ—‘ï¸  URLs auto-descartadas: 5

ğŸ’¾ Resultados JSON: audit-2025-10-26-16-41-36.json
ğŸ’¾ URLs nuevas usables: audit-2025-10-26-16-41-36-nuevas.txt
ğŸ’¾ CSV histÃ³rico actualizado: audit-history.csv
```

### Contenido JSON

```json
{
  "metadata": {
    "timestampISO": "2025-10-26T23:41:36.411Z",
    "timestampReadable": "26 de octubre de 2025, 04:41:36 p.m.",
    "inputFile": "urls-inmuebles24-2025-10-26-16-35-21-valid.txt",
    "autoDiscard": true,
    "skipHttpCheck": false,
    "durationSeconds": 18.5
  },
  "statistics": {
    "totalUrls": 130,
    "nuevas": 80,
    "nuevasUsables": 80,
    "nuevasTumbadas": 0,
    "duplicadas": 45,
    "autoDescartadas": 5
  },
  "urls": {
    "nuevas": [
      {
        "url": "https://www.inmuebles24.com/propiedades/...",
        "propertyId": "147696056",
        "httpStatus": "blocked",
        "httpStatusCode": 403,
        "isTaken": false
      }
    ],
    "duplicadas": [
      {
        "url": "https://www.inmuebles24.com/propiedades/...",
        "propertyId": "147916704",
        "duplicateInfo": {
          "propertyId": "147916704",
          "title": "Casa en Venta en Fraccionamiento Interlomas",
          "slug": "casa-en-venta-en-fraccionamiento-interlomas",
          "price": "MN 3,600,000",
          "publishedDate": "Publicado hace 10 dÃ­as",
          "url": "https://www.inmuebles24.com/...",
          "lat": 24.8392706,
          "lng": -107.4116021,
          "scrapedAt": "2025-10-26T22:16:10.644Z"
        }
      }
    ],
    "descartadas": [...]
  }
}
```

### CSV HistÃ³rico

Archivo: `audit-history.csv`

```csv
timestamp,inputFile,totalUrls,nuevas,nuevasUsables,nuevasTumbadas,duplicadas,autoDescartadas,durationSeconds,autoDiscard,skipHttpCheck
2025-10-26T23:35:00.000Z,urls-inmuebles24-...-valid.txt,130,85,80,5,45,5,18.5,true,false
2025-10-27T10:15:00.000Z,urls-inmuebles24-...-valid.txt,95,60,58,2,35,2,12.3,true,false
2025-10-27T14:22:00.000Z,urls-inmuebles24-...-valid.txt,110,70,68,2,40,2,15.1,true,false
```

**Uso del CSV:**
- AnÃ¡lisis de tendencias (duplicados crecientes = saturaciÃ³n)
- MÃ©tricas de rendimiento (durationSeconds por URL)
- ValidaciÃ³n de corridas (totalUrls != nuevas + duplicadas = error)

---

## ğŸ¤– Paso 3: Scraper Masivo

### OpciÃ³n 1: Iterador Manual

```bash
# Leer archivo lÃ­nea por lÃ­nea y scrapear
while IFS= read -r url; do
  echo ""
  node inmuebles24culiacanscraper.js "$url"
done < audit-2025-10-26-16-41-36-nuevas.txt
```

### OpciÃ³n 2: Script Automatizado (Recomendado)

Crear `scraper-batch.sh`:

```bash
#!/bin/bash

# ConfiguraciÃ³n
INPUT_FILE="$1"
MAX_URLS="${2:-10}"

if [ -z "$INPUT_FILE" ]; then
  echo "Uso: ./scraper-batch.sh <archivo-urls> [max-urls]"
  exit 1
fi

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  ğŸ¤– SCRAPER BATCH - INMUEBLES24                              â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“ Archivo: $INPUT_FILE"
echo "ğŸ“Š LÃ­mite: $MAX_URLS URLs"
echo ""

count=0
while IFS= read -r url && [ $count -lt $MAX_URLS ]; do
  count=$((count + 1))
  echo ""
  echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo "  PROCESANDO URL $count/$MAX_URLS"
  echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo ""

  echo "" | node inmuebles24culiacanscraper.js "$url"

  # Esperar 5 segundos entre propiedades
  if [ $count -lt $MAX_URLS ]; then
    echo ""
    echo "â³ Esperando 5 segundos antes de la siguiente..."
    sleep 5
  fi
done < "$INPUT_FILE"

echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  âœ… BATCH COMPLETADO                                         â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“Š Total procesadas: $count URLs"
echo ""
```

Uso:

```bash
chmod +x scraper-batch.sh
./scraper-batch.sh audit-2025-10-26-16-41-36-nuevas.txt 20
```

---

## ğŸ“ˆ HistÃ³rico y AnÃ¡lisis

### Consultar CSV HistÃ³rico

```bash
# Ver todas las corridas
cat audit-history.csv

# EstadÃ­sticas bÃ¡sicas
echo "Total corridas:"
tail -n +2 audit-history.csv | wc -l

echo "URLs procesadas totales:"
tail -n +2 audit-history.csv | cut -d',' -f3 | awk '{sum+=$1} END {print sum}'

echo "Duplicados promedio:"
tail -n +2 audit-history.csv | cut -d',' -f7 | awk '{sum+=$1; count++} END {print sum/count}'
```

### AnÃ¡lisis Avanzado (Python)

```python
import pandas as pd

# Cargar histÃ³rico
df = pd.read_csv('audit-history.csv')

# Convertir timestamp
df['timestamp'] = pd.to_datetime(df['timestamp'])

# MÃ©tricas
print("Resumen por corrida:")
print(df[['timestamp', 'totalUrls', 'nuevas', 'duplicadas']].to_string())

# Tasa de duplicados
df['tasa_duplicados'] = (df['duplicadas'] / df['totalUrls']) * 100
print(f"\nTasa de duplicados promedio: {df['tasa_duplicados'].mean():.2f}%")

# URLs por segundo
df['urls_por_segundo'] = df['totalUrls'] / df['durationSeconds']
print(f"\nVelocidad promedio: {df['urls_por_segundo'].mean():.2f} URLs/seg")
```

---

## ğŸ’¡ Casos de Uso

### Caso 1: Scraping Diario Automatizado

```bash
#!/bin/bash
# daily-scraping.sh

DATE=$(date +%Y-%m-%d)
LOG_FILE="logs/scraping-$DATE.log"

echo "ğŸš€ Inicio scraping diario: $DATE" | tee -a $LOG_FILE

# 1. Extraer URLs (5 pÃ¡ginas)
echo "ğŸ“¥ Extrayendo URLs..." | tee -a $LOG_FILE
node 1extractorurlinmuebles24.js \
  "https://www.inmuebles24.com/casas-en-venta-en-culiacan.html" \
  --max-pages 5 >> $LOG_FILE 2>&1

# 2. Auditar y auto-descartar
URLS_FILE=$(ls -t urls-inmuebles24-*.txt | head -1)
echo "ğŸ” Auditando: $URLS_FILE" | tee -a $LOG_FILE
node auditor-urls-inmuebles24.js $URLS_FILE --auto-discard >> $LOG_FILE 2>&1

# 3. Scrapear (mÃ¡ximo 20 nuevas)
AUDIT_FILE=$(ls -t audit-*-nuevas.txt | head -1)
echo "ğŸ¤– Scrapeando: $AUDIT_FILE" | tee -a $LOG_FILE
./scraper-batch.sh $AUDIT_FILE 20 >> $LOG_FILE 2>&1

echo "âœ… Scraping diario completado: $DATE" | tee -a $LOG_FILE
```

### Caso 2: Scraping por Rango de Precio

```bash
# Extraer casas de $3M - $4M
node 1extractorurlinmuebles24.js \
  "https://www.inmuebles24.com/casas-en-venta-en-culiacan-de-3000000-a-4000000-pesos.html" \
  --max-pages 10

# Auditar sin HTTP check (mÃ¡s rÃ¡pido)
node auditor-urls-inmuebles24.js \
  urls-inmuebles24-2025-10-26-18-00-00-valid.txt \
  --skip-http-check

# Scrapear todas las nuevas
./scraper-batch.sh audit-2025-10-26-18-01-00-nuevas.txt 999
```

### Caso 3: AnÃ¡lisis de SaturaciÃ³n

```bash
# Verificar si ya scrapeamos la mayorÃ­a de un rango
node 1extractorurlinmuebles24.js \
  "https://www.inmuebles24.com/casas-en-venta-en-culiacan-de-3500000-a-3550000-pesos.html" \
  --max-pages 5

node auditor-urls-inmuebles24.js \
  urls-inmuebles24-2025-10-26-19-00-00-valid.txt

# Si duplicadas > 80%, ese rango estÃ¡ saturado
```

---

## ğŸ”§ Troubleshooting

### Problema: Muchas URLs bloqueadas (403)

**Causa:** Cloudflare bloqueando HEAD requests del auditor
**SoluciÃ³n:** Usar `--skip-http-check` en auditor (el scraper las manejarÃ¡)

```bash
node auditor-urls-inmuebles24.js urls-...-valid.txt --skip-http-check
```

### Problema: CSV histÃ³rico corrupto

**Causa:** InterrupciÃ³n durante escritura
**SoluciÃ³n:** Regenerar desde JSONs individuales

```bash
# Backup
cp audit-history.csv audit-history.csv.backup

# Recrear
echo "timestamp,inputFile,totalUrls,nuevas,nuevasUsables,nuevasTumbadas,duplicadas,autoDescartadas,durationSeconds,autoDiscard,skipHttpCheck" > audit-history.csv

# Agregar datos desde JSONs
for f in audit-*.json; do
  # Extraer y agregar fila (script personalizado)
  node extract-json-to-csv.js $f >> audit-history.csv
done
```

### Problema: Alta tasa de duplicados

**Causa:** Ya scrapeamos la mayorÃ­a de propiedades en ese rango
**SoluciÃ³n:** Cambiar criterio de bÃºsqueda o ampliar rango de precio

---

## ğŸ“š Referencias

- **Extractor:** `1extractorurlinmuebles24.js`
- **Auditor:** `auditor-urls-inmuebles24.js`
- **Scraper:** `inmuebles24culiacanscraper.js`
- **Base de datos:** `inmuebles24-scraped-properties.json`

---

## âœ… Checklist Workflow Completo

- [ ] Ejecutar extractor con URL de bÃºsqueda
- [ ] Verificar stats en JSON generado (usablePercentage > 50%)
- [ ] Ejecutar auditor con archivo `-valid.txt`
- [ ] Revisar CSV histÃ³rico (tasa de duplicados < 80%)
- [ ] Ejecutar scraper batch con archivo `-nuevas.txt`
- [ ] Verificar propiedades publicadas en casasenventa.info
- [ ] Backup de audit-history.csv (semanal)

---

**Ãšltima actualizaciÃ³n:** 26 de octubre de 2025
**VersiÃ³n:** 1.0
